<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Keen&#39;s Blog</title>
  
  <subtitle>离开世界之前，一切都是过程</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-01-02T16:58:46.021Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Keen Wu</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文浅尝 | word2vec 参数学习说明</title>
    <link href="http://yoursite.com/uncategorized/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D%20|%20word2vec%20%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0%E8%AF%B4%E6%98%8E/"/>
    <id>http://yoursite.com/uncategorized/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D%20|%20word2vec%20%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0%E8%AF%B4%E6%98%8E/</id>
    <published>2019-12-30T19:13:43.269Z</published>
    <updated>2020-01-02T16:58:46.021Z</updated>
    
    <content type="html"><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote><p>word2vec 的模型和应用近年来备受关注。通过 word2vec 习得 words 的矢量表示在各 NLP 任务中都很受用。甚至由于谷歌的大牌效应，提到词向量就会想到 word2vec。但大家多把 word2vec 当作一个获取词向量的工具来使用，并未过多关心其模型本身。虽然作为语言模型而言其较为粗糙，但详细了解单词嵌入模型的参数学习过程，有助于了解此类似技术的工作机制。</p></blockquote><p>论文题目：word2vec Parameter Learning Explained<br>论文链接：<a href="https://arxiv.org/pdf/1411.2738.pdf">https://arxiv.org/pdf/1411.2738.pdf</a></p><p><img src="http://i2.tiimg.com/707213/e0c405eb410e86ac.png" width = "80%" /></p><hr><p>本文详细推导和解释了 word2vec 模型的参数更新方程，包括原始的 CBOW 和 skip-gram 模型；以及先进的优化技术，包括分层 softmax 和负采样。本文作者还提供了一个可交互的 demo 以便大家更直观地理解模型。demo: <a href="http://bit.ly/wevi-online">wevi-online</a></p><blockquote><p>这篇论文是被广为推荐的 word2vec 相关参考资料，可惜其作者 Rong Xin 于 2017 年驾驶飞机失事，永远离开了我们。</p></blockquote><a id="more"></a><h1 id="0-反向传播基础"><a href="#0-反向传播基础" class="headerlink" title="0 反向传播基础"></a>0 反向传播基础</h1><h2 id="0-1-单个单元的学习算法"><a href="#0-1-单个单元的学习算法" class="headerlink" title="0.1 单个单元的学习算法"></a>0.1 单个单元的学习算法</h2><p><img src="http://i2.tiimg.com/707213/dd228e5d7de93ca0.png" width = "50%" /></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;word2vec 的模型和应用近年来备受关注。通过 word2vec 习得 words 的矢量表示在各 NLP 任务中都很受用。甚至由于谷歌的大牌效应，提到词向量就会想到 word2vec。但大家多把 word2vec 当作一个获取词向量的工具来使用，并未过多关心其模型本身。虽然作为语言模型而言其较为粗糙，但详细了解单词嵌入模型的参数学习过程，有助于了解此类似技术的工作机制。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;论文题目：word2vec Parameter Learning Explained&lt;br&gt;论文链接：&lt;a href=&quot;https://arxiv.org/pdf/1411.2738.pdf&quot;&gt;https://arxiv.org/pdf/1411.2738.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;http://i2.tiimg.com/707213/e0c405eb410e86ac.png&quot; width = &quot;80%&quot; /&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;本文详细推导和解释了 word2vec 模型的参数更新方程，包括原始的 CBOW 和 skip-gram 模型；以及先进的优化技术，包括分层 softmax 和负采样。本文作者还提供了一个可交互的 demo 以便大家更直观地理解模型。demo: &lt;a href=&quot;http://bit.ly/wevi-online&quot;&gt;wevi-online&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;这篇论文是被广为推荐的 word2vec 相关参考资料，可惜其作者 Rong Xin 于 2017 年驾驶飞机失事，永远离开了我们。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
    
  </entry>
  
</feed>
