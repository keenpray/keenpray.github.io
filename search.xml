<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文浅尝 | word2vec 参数学习说明</title>
    <url>/uncategorized/%E8%AE%BA%E6%96%87%E6%B5%85%E5%B0%9D%20%7C%20word2vec%20%E5%8F%82%E6%95%B0%E5%AD%A6%E4%B9%A0%E8%AF%B4%E6%98%8E/</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><blockquote>
<p>word2vec 的模型和应用近年来备受关注。通过 word2vec 习得 words 的矢量表示在各 NLP 任务中都很受用。甚至由于谷歌的大牌效应，提到词向量就会想到 word2vec。但大家多把 word2vec 当作一个获取词向量的工具来使用，并未过多关心其模型本身。虽然作为语言模型而言其较为粗糙，但详细了解单词嵌入模型的参数学习过程，有助于了解此类似技术的工作机制。</p>
</blockquote>
<p>论文题目：word2vec Parameter Learning Explained<br>论文链接：<a href="https://arxiv.org/pdf/1411.2738.pdf">https://arxiv.org/pdf/1411.2738.pdf</a></p>
<p><img src="http://i2.tiimg.com/707213/e0c405eb410e86ac.png" width = "80%" /></p>
<hr>
<p>本文详细推导和解释了 word2vec 模型的参数更新方程，包括原始的 CBOW 和 skip-gram 模型；以及先进的优化技术，包括分层 softmax 和负采样。本文作者还提供了一个可交互的 demo 以便大家更直观地理解模型。demo: <a href="http://bit.ly/wevi-online">wevi-online</a></p>
<blockquote>
<p>这篇论文是被广为推荐的 word2vec 相关参考资料，可惜其作者 Rong Xin 于 2017 年驾驶飞机失事，永远离开了我们。</p>
</blockquote>
<a id="more"></a>
<h1 id="0-反向传播基础"><a href="#0-反向传播基础" class="headerlink" title="0 反向传播基础"></a>0 反向传播基础</h1><h2 id="0-1-单个单元的学习算法"><a href="#0-1-单个单元的学习算法" class="headerlink" title="0.1 单个单元的学习算法"></a>0.1 单个单元的学习算法</h2><p><img src="http://i2.tiimg.com/707213/dd228e5d7de93ca0.png" width = "50%" /></p>
]]></content>
  </entry>
</search>
